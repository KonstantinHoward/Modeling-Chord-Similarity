{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebd2bc10",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302110c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import pickle\n",
    "from matplotlib.pyplot import figure\n",
    "from IPython.core.debugger import set_trace\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.special import expit\n",
    "from scipy.stats import norm\n",
    "import json\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "pio.renderers.default = 'iframe'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e6935e",
   "metadata": {},
   "source": [
    "# Load experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c5d84ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/shepdyads-data/psynet/data/node.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n6/vmgch5rs4r9541qvkvbwpbjh0000gp/T/ipykernel_43855/3289960687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mNBOOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"data/{APPNAME}-data/psynet/data/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"node.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"info.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnetworks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"network.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/shepdyads-data/psynet/data/node.csv'"
     ]
    }
   ],
   "source": [
    "APPNAME = 'shepdyads' #shepdyadscls or shepdyads\n",
    "FILTER_EXPERIENCE = True\n",
    "EXPERIENCE_THRESHOLD = 0\n",
    "NBOOT = 1000\n",
    "PATH = f\"data/{APPNAME}-data/psynet/data/\"\n",
    "nodes = pd.read_csv(PATH + \"node.csv\", low_memory=False)\n",
    "infos = pd.read_csv(PATH + \"info.csv\", low_memory=False)\n",
    "networks = pd.read_csv(PATH + \"network.csv\", low_memory=False)\n",
    "participants = pd.read_csv(PATH + \"participant.csv\", low_memory=False)\n",
    "questions = pd.read_csv(PATH + \"response.csv\", low_memory=False)\n",
    "\n",
    "MIN_INTERVAL = 60\n",
    "MAX_INTERVAL = 71\n",
    "list_of_chords = [json.dumps([i,j]) for i in range(MIN_INTERVAL,MAX_INTERVAL+1) for j in range(i,MAX_INTERVAL+1)]\n",
    "\n",
    "\n",
    "# filter networks\n",
    "network_data = networks\n",
    "network_data = network_data[network_data[\"role\"] == \"experiment\"]\n",
    "network_data = network_data[network_data[\"failed\"] == 'f']\n",
    "network_data = network_data[network_data[\"trial_maker_id\"] == 'main_experiment']\n",
    "\n",
    "experiment_net_id = list(network_data['id'].to_numpy())\n",
    "\n",
    "# filter participants\n",
    "valid_participants = participants\n",
    "valid_participants = valid_participants[valid_participants[\"complete\"] == \"t\"]\n",
    "\n",
    "# filter based on musical expertise\n",
    "if FILTER_EXPERIENCE:\n",
    "    musical_experience = questions[questions[\"question\"] == \"years_playing_music\"]\n",
    "    musical_experience = musical_experience[['participant_id','answer']]\n",
    "    musical_experience = musical_experience.assign(\n",
    "        numeric_response = lambda dataframe: dataframe['answer'].map(lambda answer: json.loads(answer))\n",
    "    )\n",
    "    musical_experience = musical_experience[pd.to_numeric(musical_experience['numeric_response'], errors='coerce').notnull()]\n",
    "    musical_experience['numeric_response'] = musical_experience['numeric_response'].astype(float) #int\n",
    "    musical_experience = musical_experience[musical_experience['numeric_response'] >= EXPERIENCE_THRESHOLD]\n",
    "    experienced_ids = list(musical_experience[\"participant_id\"].to_numpy())\n",
    "    valid_participants = valid_participants[valid_participants[\"id\"].isin(experienced_ids)]\n",
    "\n",
    "valid_participant_id = list(valid_participants['id'].to_numpy())\n",
    "\n",
    "# filter trials\n",
    "trial_data = infos\n",
    "trial_data = trial_data[trial_data[\"type\"] == \"custom_trial\"]\n",
    "trial_data = trial_data[trial_data[\"failed\"] == \"f\"]\n",
    "trial_data = trial_data[trial_data[\"complete\"] == \"t\"]\n",
    "trial_data = trial_data[trial_data[\"is_repeat_trial\"] == \"f\"]\n",
    "trial_data = trial_data[trial_data[\"network_id\"].isin(experiment_net_id)]\n",
    "trial_data = trial_data[trial_data[\"participant_id\"].isin(valid_participant_id)]\n",
    "trial_data = trial_data[[\"id\", \"origin_id\", \"response_id\", \"network_id\", \"participant_id\", \"definition\", \"answer\"]]\n",
    "\n",
    "# create response summary\n",
    "\n",
    "def extract_from_json(definition, key):\n",
    "    definition = json.loads(definition)\n",
    "    return definition[key]\n",
    "\n",
    "origin_id = trial_data[\"origin_id\"].to_numpy()\n",
    "participant_id = trial_data[\"participant_id\"].to_numpy()\n",
    "response_id = trial_data[\"response_id\"].to_numpy()\n",
    "definition = trial_data[\"definition\"].to_numpy()\n",
    "definition = [json.loads(el)[\"chords\"] for el in trial_data[\"definition\"].to_numpy()]\n",
    "answer = trial_data[\"answer\"].to_numpy()\n",
    "answer = [json.loads(ans) for ans in answer]\n",
    "\n",
    "synth = [el[\"synth\"] for el in definition]\n",
    "chord_1 = [el[\"dyad_class_1\"] for el in definition]\n",
    "chord_2 = [el[\"dyad_class_2\"] for el in definition]\n",
    "idx_1 = [el[\"idx_1\"] for el in definition]\n",
    "idx_2 = [el[\"idx_2\"] for el in definition]\n",
    "\n",
    "for i in range(len(chord_1)):\n",
    "    c_1 = chord_1[i]\n",
    "    c_2 = chord_2[i]\n",
    "    i_1 = idx_1[i]\n",
    "    i_2 = idx_2[i]\n",
    "    if i_1 > i_2:\n",
    "        chord_1[i] = c_2\n",
    "        chord_2[i] = c_1\n",
    "        idx_1[i] = i_2\n",
    "        idx_2[i] = i_1\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    \"origin_id\": origin_id,\n",
    "    \"participant_id\": participant_id,\n",
    "    \"response_id\": response_id,\n",
    "    \"answer\": answer,\n",
    "    \"synth\": synth,\n",
    "    \"chord_1\": chord_1,\n",
    "    \"chord_2\": chord_2,\n",
    "    \"idx_1\": idx_1,\n",
    "    \"idx_2\": idx_2\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce675dc0-6192-4ce9-9d80-6904806a0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a88ac6-f489-4123-8a0b-81b25ceb5439",
   "metadata": {},
   "source": [
    "# Costs and demograhics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb798a5-d0ff-4c66-96cb-88d86896afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = pd.read_csv(PATH + \"participant.csv\", low_memory=False)\n",
    "valid_participants = participants[participants[\"complete\"] == \"t\"]\n",
    "experiment_summary = {\n",
    "    \"N_participants\": valid_participants.shape[0],\n",
    "    \"cost\": participants[\"base_pay\"].sum() + participants[\"bonus\"].sum()\n",
    "}\n",
    "\n",
    "experiment_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa933eb-4e3a-4433-b1ca-3e2a8244ccd7",
   "metadata": {},
   "source": [
    "# Export to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b523c738-33f0-4942-a361-e25fe4463d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"data_dyads.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2ae85",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fb4a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_SCORE = True\n",
    "\n",
    "def preprocess_data(data, list_of_chords, z_score):\n",
    "    N = len(list_of_chords) # 81\n",
    "    sim_mat = np.zeros(shape=(N,N))\n",
    "\n",
    "    if not z_score:\n",
    "        processed = data[[\"idx_1\",\"idx_2\",\"answer\"]]\n",
    "        processed = processed.groupby([\"idx_1\",\"idx_2\"],as_index=False).mean()\n",
    "\n",
    "        for i in range(len(processed.index)):\n",
    "            idx1 = processed[\"idx_1\"][i]\n",
    "            idx2 = processed[\"idx_2\"][i]\n",
    "            sim_mat[idx1, idx2] = processed[\"answer\"][i]\n",
    "\n",
    "        sim_mat = sim_mat / 6\n",
    "        sim_mat = sim_mat + np.transpose(sim_mat)\n",
    "        np.fill_diagonal(sim_mat,1)\n",
    "\n",
    "    else:\n",
    "        processed = data[[\"idx_1\",\"idx_2\",\"participant_id\",\"answer\"]]\n",
    "        processed = processed.dropna()\n",
    "        gb = processed.groupby('participant_id') \n",
    "        splitted_data = [gb.get_group(x) for x in gb.groups]\n",
    "        splitted_data_z = []\n",
    "        for df in splitted_data:\n",
    "            z_data = scale(df['answer'].to_numpy())\n",
    "            updated_df = df.assign(z_answer = z_data)\n",
    "            splitted_data_z = splitted_data_z + [updated_df]\n",
    "        processed = pd.concat(splitted_data_z)\n",
    "        processed = processed.groupby([\"idx_1\",\"idx_2\"],as_index=False).mean()\n",
    "\n",
    "        for i in range(len(processed.index)):\n",
    "            idx1 = processed[\"idx_1\"][i]\n",
    "            idx2 = processed[\"idx_2\"][i]\n",
    "            sim_mat[idx1, idx2] = processed[\"z_answer\"][i]\n",
    "\n",
    "        sim_mat = norm.cdf(sim_mat + np.transpose(sim_mat))\n",
    "        # sim_mat = sim_mat + np.transpose(sim_mat)\n",
    "        np.fill_diagonal(sim_mat,1)\n",
    "\n",
    "\n",
    "    experimental_data = {\"similarity\": sim_mat}\n",
    "    \n",
    "    return experimental_data\n",
    "\n",
    "experimental_data = preprocess_data(data, list_of_chords, Z_SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291fdce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 1)\n",
    "fig.set_figheight(10)\n",
    "fig.set_figwidth(10)\n",
    "\n",
    "count = 0\n",
    "\n",
    "for key in experimental_data.keys():\n",
    "    idxs = count\n",
    "    axs.matshow(experimental_data[key])\n",
    "    axs.set_title(key, fontsize=20)\n",
    "    axs.xaxis.set_visible(False)\n",
    "    axs.yaxis.set_visible(False)\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b62afdd",
   "metadata": {},
   "source": [
    "# Compute MDS embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = {}\n",
    "n_components = 3\n",
    "np.random.seed(1233) #1233\n",
    "\n",
    "for key in experimental_data.keys():\n",
    "    dissimilarity = 1 - experimental_data[key]\n",
    "    \n",
    "    embedding = MDS(n_components=n_components, metric = True, dissimilarity = \"precomputed\", max_iter = 10000, eps = 1e-100)\n",
    "    init = embedding.fit_transform(dissimilarity)\n",
    "\n",
    "    embedding = MDS(n_components=n_components, metric = False, dissimilarity = \"precomputed\", max_iter = 10000, eps = 1e-100)\n",
    "    transformed = embedding.fit_transform(dissimilarity, init = init)\n",
    "    \n",
    "    transformed_data[key] = transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8581a9cd",
   "metadata": {},
   "source": [
    "# Plot MDS embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8838c75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations = list_of_chords\n",
    "\n",
    "X = transformed_data[\"similarity\"][:,0]\n",
    "Y = transformed_data[\"similarity\"][:,1]\n",
    "Z = transformed_data[\"similarity\"][:,2]\n",
    "\n",
    "\n",
    "val = []\n",
    "for l in list_of_chords:\n",
    "    cdist = np.abs(np.diff(json.loads(l))[0])\n",
    "    if cdist > 6:\n",
    "        cdist = 12 - cdist\n",
    "    val.append(cdist)\n",
    "df = pd.DataFrame({\"X\": X, \"Y\": Y, \"Z\": Z, \"cat\": list_of_chords, \"val\": val})\n",
    "fig = px.scatter_3d(df, x='X', y='Y', z='Z', text=\"cat\", color='val', width=1000, height=1000,\n",
    "                        title=\"MDS Space of Dyads\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5e396b-0c21-442a-b1ce-35bbe91c558b",
   "metadata": {},
   "source": [
    "# Plot transition difference against similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300bb69f-3708-4d83-ad98-007bad44b1c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from scipy import stats\n",
    "\n",
    "def compute_wrapped_distance(chord1, chord2, p): \n",
    "    diffs1 = np.mod(np.abs(np.array(chord1) - np.array(chord2)), 12)\n",
    "    for i, diff in enumerate(diffs1):\n",
    "        if diff > 6:\n",
    "            diffs1[i] = 12 - diff\n",
    "    diffs2 = np.mod(np.abs(np.flip(np.array(chord1)) - np.array(chord2)), 12)\n",
    "    for i, diff in enumerate(diffs2):\n",
    "        if diff > 6:\n",
    "            diffs2[i] = 12 - diff\n",
    "    diffs1 = diffs1**p\n",
    "    diffs2 = diffs2**p\n",
    "    diffs1 = np.power(diffs1, p)\n",
    "    diffs2 = np.power(diffs2, p)\n",
    "    dist = min(np.sum(diffs1), np.sum(diffs2))\n",
    "    return dist ** (1/p)\n",
    "\n",
    "def compute_distance(c1, c2, p):\n",
    "    v = np.sum(np.abs(c1-c2)**p)\n",
    "    return v ** 1/p \n",
    "\n",
    "\n",
    "chord_sim = experimental_data[\"similarity\"]\n",
    "\n",
    "dists = []\n",
    "sims = []\n",
    "P = 1\n",
    "for i in range(len(list_of_chords)):\n",
    "    for j in range(i+1,len(list_of_chords)):\n",
    "        diff = compute_distance(np.array(json.loads(list_of_chords[i])), np.array(json.loads(list_of_chords[j])),P)\n",
    "        sim = chord_sim[i,j]\n",
    "        dists = dists + [diff]\n",
    "        sims = sims + [sim]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "ax.scatter(dists,sims)\n",
    "ax.set_xlabel(\"L1 distance\", fontsize = 20)\n",
    "ax.set_ylabel(\"Similarity\", fontsize = 20)\n",
    "\n",
    "print(stats.pearsonr(dists, sims))\n",
    "\n",
    "mean_sim = []\n",
    "cond_distance = []\n",
    "for i in range(int(min(dists)), int(max(dists)) + 1):\n",
    "    dvals = np.array([int(d) for d in dists])\n",
    "    dsims = np.array(sims)\n",
    "    mean_sim = mean_sim + [np.mean(dsims[dvals == i])]\n",
    "    cond_distance = cond_distance + [i]\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "ax.scatter(cond_distance,mean_sim)\n",
    "ax.set_xlabel(\"L1 distance\", fontsize = 20)\n",
    "ax.set_ylabel(\"Mean Similarity\", fontsize = 20)\n",
    "\n",
    "dists = []\n",
    "sims = []\n",
    "P = 1\n",
    "for i in range(len(list_of_chords)):\n",
    "    for j in range(i+1,len(list_of_chords)):\n",
    "        diff = compute_wrapped_distance(np.array(json.loads(list_of_chords[i])), np.array(json.loads(list_of_chords[j])), P)\n",
    "        sim = chord_sim[i,j]\n",
    "        dists = dists + [diff]\n",
    "        sims = sims + [sim]\n",
    "\n",
    "print(stats.pearsonr(dists, sims))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "ax.scatter(dists,sims)\n",
    "ax.set_xlabel(\"Wrapped L1 distance\", fontsize = 20)\n",
    "ax.set_ylabel(\"Similarity\", fontsize = 20)\n",
    "\n",
    "mean_sim = []\n",
    "sd_sim = []\n",
    "cond_distance = []\n",
    "for i in range(int(min(dists)), int(max(dists)) + 1):\n",
    "    dvals = np.array([int(d) for d in dists])\n",
    "    dsims = np.array(sims)\n",
    "    mean_sim = mean_sim + [np.mean(dsims[dvals == i])]\n",
    "    sd_sim = sd_sim + [np.std(dsims[dvals == i]) / np.sqrt(len(dsims[dvals == i]))] # std of mean\n",
    "    cond_distance = cond_distance + [i]\n",
    "    \n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "ax.scatter(cond_distance,mean_sim)\n",
    "ax.errorbar(cond_distance,mean_sim,1.96*np.array(sd_sim))\n",
    "ax.set_xlabel(\"Wrapped L1 distance\", fontsize = 20)\n",
    "ax.set_ylabel(\"Mean Similarity\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0720857b-02b1-4aeb-8940-ce6107a1d7ec",
   "metadata": {},
   "source": [
    "# Simulating similarity matrix based on wrapped L1 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5cb073-5270-43e8-a424-761a65a87eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_sim_mat = np.zeros((len(list_of_chords), len(list_of_chords)))\n",
    "for i in range(len(list_of_chords)):\n",
    "    for j in range(len(list_of_chords)):\n",
    "        simulated_sim_mat[i,j] = compute_wrapped_distance(np.array(json.loads(list_of_chords[i])), np.array(json.loads(list_of_chords[j])), P)\n",
    "\n",
    "fig, ax = plt.subplots(1,2,figsize = (12,12))\n",
    "ax[0].matshow(1 - simulated_sim_mat / np.max(simulated_sim_mat))\n",
    "ax[1].matshow(experimental_data[\"similarity\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2087eaca-aa73-4218-b320-04d733d32933",
   "metadata": {},
   "source": [
    "# Minimal Voice-leading Distance Analysis\n",
    "Run a linear regression with the calculated wrapped distances and perceived similarities. Compare similarities predicted by the model with perceived distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4e708-6844-4a67-8f84-4a2835b4ed26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fit = scipy.stats.linregress(dists, sims)\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "\n",
    "pred_sims = np.multiply(dists, fit[0]) + fit[1]\n",
    "ax.plot(dists, pred_sims, color = \"RED\")\n",
    "ax.scatter(dists, sims, alpha = 0.3)\n",
    "\n",
    "dist_var = np.var(abs(pred_sims-sims))\n",
    "ax.set_xlabel(\"Wrapped L1 distance\", fontsize = 20)\n",
    "ax.set_ylabel(\"Similarity\", fontsize = 20)\n",
    "print(\"Slope: \", fit[0], \", y-intercept: \", fit[1], \"r-value: \", fit[2], \"\\n p-value: \", fit[3], \"variance: \", dist_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5a28a0",
   "metadata": {},
   "source": [
    "# Spectral Pitch-class Distance Implementation\n",
    "Implement spectral pitch-class distance function. Calculate distance for every pair of chords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a40577b-d741-4f81-b2d5-dafd52550bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral Pitch-class Distance\n",
    "C = 60\n",
    "harmonic_series = np.array([0, 1200, 700, 500, 400, 300, 300, 200, 200, 200, 200, 100])\n",
    "\n",
    "# caclulates spectral distance between two chords\n",
    "# rho = shrinking factor of successive harmonics\n",
    "# sigma = variance of smeared distribution\n",
    "def spectral_dist(chord1, chord2, rho, sigma) :\n",
    "    # for each note, get row vector representation\n",
    "    x_wt = np.zeros(1200)\n",
    "    y_wt = np.zeros(1200)\n",
    "    for i in range(2) :\n",
    "        x_wt = x_wt + note_to_vec(chord1[i], rho)\n",
    "        y_wt = y_wt + note_to_vec(chord2[i], rho)\n",
    "        \n",
    "    smear = np.random.normal(1, sigma, 1200)\n",
    "    x_e = np.convolve(x_wt, smear,'same' )\n",
    "    y_e = np.convolve(y_wt, smear, 'same')\n",
    "    \n",
    "    xy = x_e @ y_e\n",
    "    xx = x_e @ x_e\n",
    "    yy = y_e @ y_e\n",
    "    \n",
    "    return 1 - (xy / np.sqrt(xx * yy))\n",
    "    \n",
    "# converts single tone to frequency vector representation\n",
    "def note_to_vec(note, rho) :\n",
    "    idx = ((note - C) * 100) % 1200\n",
    "    harm = np.zeros(1200)\n",
    "    for i in range(len(harmonic_series)) :\n",
    "        idx += harmonic_series[i]\n",
    "        idx %= 1200\n",
    "        harm[idx] = (i+1)**-rho\n",
    "    return harm\n",
    "        \n",
    "# compute spectral distances\n",
    "specs = []\n",
    "rho = 0.6\n",
    "sigma = 1\n",
    "for i in range(len(list_of_chords)):\n",
    "    for j in range(i+1, len(list_of_chords)):\n",
    "        spec = spectral_dist(np.array(json.loads(list_of_chords[i])), np.array(json.loads(list_of_chords[j])), rho, sigma)\n",
    "        specs = specs + [spec]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efd13df",
   "metadata": {},
   "source": [
    " # Spectral Optimization and Analysis\n",
    " Optimizing for the hyperparameters rho and sigma was not successful (commented out). Once again, run a linear regression with the calculated distances and perceived similarities. Compare similarities predicted by the model with perceived distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43523796",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = scipy.optimize.Bounds(lb = [0.1,0.1], ub = [0.6, 1], keep_feasible = [True,True])  \n",
    "\n",
    "\n",
    "def spectral_opt(params) :\n",
    "    specs = []\n",
    "    for i in range(len(list_of_chords)):\n",
    "        for j in range(i+1, len(list_of_chords)):\n",
    "            spec = spectral_dist(np.array(json.loads(list_of_chords[i])), np.array(json.loads(list_of_chords[j])),\n",
    "                    params[0], params[1])\n",
    "            specs = specs + [spec]\n",
    "    fit = scipy.stats.linregress(specs, sims)\n",
    "    spec_sims = np.multiply(specs, fit[0]) + fit[1]\n",
    "    \n",
    "    \n",
    "    return np.linalg.norm(spec_sims-sims)\n",
    "\n",
    "guess_params = [0.5, 0.9]\n",
    "#result = minimize(spectral_opt, guess_params, bounds = b)\n",
    "\n",
    "#print(result)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "ax.scatter(specs, sims, alpha = 0.2)\n",
    "\n",
    "ax.set_xlabel(\"Spectral Pitch-class Distance\", fontsize = 20)\n",
    "ax.set_ylabel(\"Similarity\", fontsize = 20)\n",
    "print(\"Slope: \", fit[0], \", y-intercept: \", fit[1], \"r-value: \", fit[2], \n",
    "      \"\\n p-value: \", fit[3], \"variance: \", spec_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9d2835",
   "metadata": {},
   "source": [
    "# Consonance Difference\n",
    "Implement consonance difference function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = [\"answer\",\"intervals\"]\n",
    "consonance_dyads = pd.read_csv(r\"data/consonance_judgments_dyads.csv\", usecols=col_names, low_memory=False)\n",
    "\n",
    "# intervals are strings, answers are numpy.float64\n",
    "consonance_dyads = consonance_dyads.groupby('intervals').mean().reset_index()\n",
    "\n",
    "\n",
    "# given two dyads, find the difference in their consonance ratings\n",
    "def cons_dif(chord1, chord2, cons) :\n",
    "    cons1 = cons.loc[cons[\"intervals\"]==chord1][\"answer\"].tolist()[0]\n",
    "    cons2 = cons.loc[cons[\"intervals\"]==chord2][\"answer\"].tolist()[0]\n",
    "    return cons1-cons2\n",
    "\n",
    "# compute differences for every combination of chords\n",
    "diffs = []\n",
    "for i in range(len(list_of_chords)):\n",
    "    for j in range(i+1, len(list_of_chords)):\n",
    "        diff = cons_dif(list_of_chords[i], list_of_chords[j], consonance_dyads)\n",
    "        diffs = diffs +[diff]\n",
    "        \n",
    "        \n",
    "# to get number of participants\n",
    "part =  pd.read_csv(r\"data/consonance_judgments_dyads.csv\", usecols=['participant_id'], low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6468f",
   "metadata": {},
   "source": [
    "# Absolute Consonance Difference Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e31c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_diffs = np.abs(diffs)\n",
    "\n",
    "fit = scipy.stats.linregress(abs_diffs, sims)\n",
    "\n",
    "\n",
    "abs_diff_sims = np.multiply(abs_diffs, fit[0]) + fit[1]\n",
    "\n",
    "cons_abs_var = np.var(abs(abs_diff_sims-sims))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "ax.scatter(abs_diffs, sims, alpha = 0.2)\n",
    "ax.plot(abs_diffs, abs_diff_sims, color=\"RED\")\n",
    "ax.set_xlabel(\"Absolute Difference in Consonance Rating\", fontsize = 20)\n",
    "ax.set_ylabel(\"Similarity\", fontsize = 20)\n",
    "print(\"Slope: \", fit[0], \", y-intercept: \", fit[1], \"r-value: \", fit[2], \n",
    "      \"\\n p-value: \", fit[3], \"variance: \", cons_abs_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fe9f55",
   "metadata": {},
   "source": [
    "# Gaussian Consonance Difference Optimization and Analysis\n",
    "Optimizing for sigma did not improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe72142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# objective function to optimize sigma\n",
    "def gauss_opt(sigma) :\n",
    "    gaussian_diffs = np.exp(-np.power(diffs,2)) / sigma\n",
    "    fit = scipy.stats.linregress(gaussian_diffs, sims)\n",
    "    gsims = np.multiply(gaussian_diffs, fit[0]) + fit[1]\n",
    "    return np.linalg.norm(gsims - sims)\n",
    "\n",
    "guess_sigma = 1\n",
    "result = minimize(gauss_opt, guess_sigma)\n",
    "\n",
    "# Gaussian differences in consonance rating\n",
    "best_gauss = np.exp(-np.power(diffs,2)) / result['x'][0]\n",
    "\n",
    "\n",
    "fit = scipy.stats.linregress(best_gauss, sims)\n",
    "\n",
    "# similarities predicted by optimized Gaussian difference\n",
    "gauss_sims = np.multiply(best_gauss, fit[0]) + fit[1]\n",
    "\n",
    "cons_gauss_var = np.var(abs(gauss_sims-sims))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "ax.scatter(best_gauss, sims, alpha = 0.2)\n",
    "ax.plot(best_gauss, gauss_sims, color=\"RED\")\n",
    "ax.set_xlabel(\"Gaussian Difference in Consonance\", fontsize = 20)\n",
    "ax.set_ylabel(\"Similarity\", fontsize = 20)\n",
    "print(\"Slope: \", fit[0], \", y-intercept: \", fit[1], \", r-value: \", fit[2], \n",
    "      \",\\n p-value: \", fit[3], \", variance: \", cons_gauss_var, \", optimized sigma:\", result['x'][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ede1f3",
   "metadata": {},
   "source": [
    "# Sigmoid Consonance Difference Analysis\n",
    "Unhelpful data transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c6e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_diffs = 1 / (1 + np.exp(np.multiply(diffs,-1)))\n",
    "\n",
    "fit = scipy.stats.linregress(sigmoid_diffs, sims)\n",
    "\n",
    "diff_sims = np.multiply(sigmoid_diffs, fit[0]) + fit[1]\n",
    "\n",
    "cons_sig_var = np.var(abs(diff_sims-sims))\n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "ax.scatter(sigmoid_diffs, sims, alpha = 0.2)\n",
    "ax.plot(sigmoid_diffs, diff_sims, color=\"RED\")\n",
    "ax.set_xlabel(\"Sigmoid Difference in Consonance Rating\", fontsize = 20)\n",
    "ax.set_ylabel(\"Similarity\", fontsize = 20)\n",
    "print(\"Slope: \", fit[0], \", y-intercept: \", fit[1], \", r-value: \", fit[2], \n",
    "      \",\\n p-value: \", fit[3], \", variance: \", cons_sig_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9a811e",
   "metadata": {},
   "source": [
    "# Combined Model Analysis\n",
    "Linear regression of two variables using minimal voice-leading distance and Gaussian consonance difference data as predictors for perceived similarity data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf62e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import linear_model\n",
    "mixture = linear_model.LinearRegression()\n",
    "# use Gaussian differences\n",
    "mixture.fit(np.array([dists,best_gauss]).T, sims)\n",
    "\n",
    "mix_pred_sims = np.multiply(dists,mixture.coef_[0]) + np.multiply(best_gauss,mixture.coef_[1]) + mixture.intercept_\n",
    "mix_r = np.sqrt(mixture.score(np.array([dists,best_gauss]).T, sims))\n",
    "mix_var = np.var(abs(mix_pred_sims-sims))\n",
    "\n",
    "chord_pairs = []\n",
    "for i in range(len(list_of_chords)):\n",
    "    for j in range(i+1, len(list_of_chords)):\n",
    "        pair = [list_of_chords[i], list_of_chords[j]]\n",
    "        chord_pairs = chord_pairs + [pair]\n",
    "\n",
    "\n",
    "mix_df = pd.DataFrame({\"Wrapped Distance\" : dists, \"Consonance Difference\" : best_gauss, \"Pair\" : chord_pairs, \n",
    "                       \"Similarity\" : sims})\n",
    "fig = px.scatter_3d(mix_df, x='Wrapped Distance', y='Consonance Difference', \n",
    "                    z='Similarity', color = 'Similarity', width=1000, \n",
    "                    height=1000, title='Multivariate Fit' )\n",
    "fig.show()\n",
    "\n",
    "\n",
    "print(\"Distance contribution: \", mixture.coef_[0], \", Gaussian Consonance Difference contribution: \", \n",
    "      mixture.coef_[1], \", \\n r: \", mix_r, \", variance: \", mix_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a55a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "ax.scatter(dists, best_gauss,c=mix_pred_sims,  alpha = 0.8)\n",
    "ax.set_xlabel(\"Wrapped Distance\", fontsize = 20)\n",
    "ax.set_ylabel(\"Gaussian Consonance Difference\", fontsize = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635645a",
   "metadata": {},
   "source": [
    "# Standard Voice-leading Distance Analysis\n",
    "As a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_sim = experimental_data[\"similarity\"]\n",
    "\n",
    "dists = []\n",
    "sims = []\n",
    "P = 1\n",
    "for i in range(len(list_of_chords)):\n",
    "    for j in range(i+1,len(list_of_chords)):\n",
    "        diff = compute_distance(np.array(json.loads(list_of_chords[i])), np.array(json.loads(list_of_chords[j])),P)\n",
    "        sim = chord_sim[i,j]\n",
    "        dists = dists + [diff]\n",
    "        sims = sims + [sim]\n",
    "        \n",
    "fit = scipy.stats.linregress(dists, sims)\n",
    "\n",
    "st_sims = np.multiply(dists, fit[0]) + fit[1]\n",
    "\n",
    "st_var = np.var(abs(st_sims-sims))\n",
    "\n",
    "fig, ax = plt.subplots(1,1,figsize = (7,7))\n",
    "ax.scatter(dists, sims, alpha = 0.2)\n",
    "ax.plot(dists, st_sims, color=\"RED\")\n",
    "ax.set_xlabel(\"L1 Distance\", fontsize = 20)\n",
    "ax.set_ylabel(\"Similarity\", fontsize = 20)\n",
    "print(\"Slope: \", fit[0], \", y-intercept: \", fit[1], \", r-value: \", fit[2], \n",
    "      \",\\n p-value: \", fit[3], \", variance: \", st_var)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
